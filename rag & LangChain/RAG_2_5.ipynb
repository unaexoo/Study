{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2-5. RAG-Vector Store\n",
    "벡터 저장소(Vector Store)는 벡터 형태로 표현된 데이터, 즉 임베딩 벡터들을 효율적으로 저장하고 검색할 수 있는 시스템이나 데이터베이스를 의미   \n",
    "- 벡터 저장\n",
    "- 벡터 검색\n",
    "- 결과 반환\n",
    "\n",
    "### 2-5-1. Chroma\n",
    "- 임베딩 및 메타데이터 저장: 대규모의 임베딩 데이터와 이와 관련된 메타데이터를 효율적으로 저장할 수 있음\n",
    "- 문서 및 쿼리 임베딩: 텍스트 데이터를 벡터 공간에 매핑하여 임베딩을 생성할 수 있으며, 이를 통해 검색 작업이 가능\n",
    "- 임베딩 검색: 사용자 쿼리에 기반하여 가장 관련성 높은 임베딩을 찾아내는 검색 기능을 제공\n",
    "\n",
    "**1. 유사도 기반 검색(Similarity search)**\n",
    "\n",
    "Chroma 벡터 저장소를 사용하여 대규모 텍스트 데이터셋에서 빠르고 효율적으로 유사도 기반 검색(Similarity search)을 수행\n",
    "\n",
    "1. 데이터 로드\n",
    "\n",
    "2. 텍스트 분할\n",
    "- RecursiveCharacterTextSplitter를 사용하여 로드된 텍스트를 여러 개의 작은 조각으로 분할\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'한국의 역사는 수천 년에 걸쳐 이어져 온 긴 여정 속에서 다양한 문화와 전통이 형성되고 발전해 왔습니다. 고조선에서 시작해 삼국 시대의 경쟁, 그리고 통일 신라와 고려를 거쳐 조선까지, 한반도는 많은 변화를 겪었습니다.\\n\\n고조선은 기원전 2333년 단군왕검에 의해 세워졌다고 전해집니다. 이는 한국 역사상 최초의 국가로, 한민족의 시원이라 할 수 있습니다. 이후 기원전 1세기경에는 한반도와 만주 일대에서 여러 소국이 성장하며 삼한 시대로 접어듭니다.'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "loader = TextLoader('history.txt',encoding='utf-8')\n",
    "data = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=250,\n",
    "    chunk_overlap=50,\n",
    "    encoding_name='cl100k_base'\n",
    ")\n",
    "\n",
    "texts = text_splitter.split_text(data[0].page_content)\n",
    "texts[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. 임베딩 모델 초기화 \n",
    "4. Chroma 벡터 저장소 생성\n",
    "- Chroma.from_texts 메소드를 사용하여 분할된 텍스트들을 임베딩하고, 이 임베딩을 Chroma 벡터 저장소에 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Public\\Documents\\ESTsoft\\CreatorTemp\\ipykernel_12500\\2394470721.py:1: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings_model = HuggingFaceEmbeddings(\n",
      "c:\\langchain\\venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.chroma.Chroma at 0x2a625503250>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_model = HuggingFaceEmbeddings(\n",
    "    model_name='BAAI/bge-m3',\n",
    "    model_kwargs={'device':'cuda'},\n",
    "    encode_kwargs={'normalize_embeddings':True},\n",
    ")\n",
    "\n",
    "db = Chroma.from_texts(\n",
    "    texts,\n",
    "    embeddings_model,\n",
    "    collection_name='history',\n",
    "    persist_directory='./db/chromadb',\n",
    "    collection_metadata = {'hnsw:space': 'cosine'},\n",
    ")\n",
    "\n",
    "db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. 유사도 기반 검색 수행\n",
    "- query 변수에 검색 쿼리를 정의\n",
    "- db.similarity_search 메소드를 사용하여 저장된 데이터 중에서 쿼리와 가장 유사한 문서를 찾음\n",
    "- 검색 결과를 docs 변수에 저장하고, 가장 유사한 문서의 내용은 docs[0].page_content를 통해 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "조선은 1392년 이성계에 의해 건국되어, 1910년까지 이어졌습니다. 조선 초기에는 세종대왕이 한글을 창제하여 백성들의 문해율을 높이는 등 문화적, 과학적 성취가 이루어졌습니다. 그러나 조선 후기에는 내부적으로 실학의 발전과 함께 사회적 변화가 모색되었으나, 외부로부터의 압력은 점차 커져만 갔습니다.\n"
     ]
    }
   ],
   "source": [
    "query = '누가 한글을 창제했나요?'\n",
    "docs = db.similarity_search(query)\n",
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** 2. MMR(Maximum marginal relevance search)**\n",
    "\n",
    "최대 한계 관련성(Maximum Marginal Relevance, MMR) 검색 방식은 유사성과 다양성의 균형을 맞추어 검색 결과의 품질을 향상시키는 알고리즘   \n",
    "검색 쿼리에 대한 문서들의 관련성을 최대화하는 동시에, 검색된 문서들 사이의 중복성을 최소화하여, 사용자에게 다양하고 풍부한 정보를 제공하는 것을 목표   \n",
    "\n",
    "MMR은 쿼리에 대한 각 문서의 유사성 점수와 이미 선택된 문서들과의 다양성(또는 차별성) 점수를 조합하여, 각 문서의 최종 점수를 계산   \n",
    "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\">\n",
    "  <mtext>MMR</mtext>\n",
    "  <mo>=</mo>\n",
    "  <mi>&#x3BB;</mi>\n",
    "  <mo>&#x22C5;</mo>\n",
    "  <mtext>Sim</mtext>\n",
    "  <mo stretchy=\"false\">(</mo>\n",
    "  <mi>d</mi>\n",
    "  <mo>,</mo>\n",
    "  <mi>Q</mi>\n",
    "  <mo stretchy=\"false\">)</mo>\n",
    "  <mo>&#x2212;</mo>\n",
    "  <mo stretchy=\"false\">(</mo>\n",
    "  <mn>1</mn>\n",
    "  <mo>&#x2212;</mo>\n",
    "  <mi>&#x3BB;</mi>\n",
    "  <mo stretchy=\"false\">)</mo>\n",
    "  <mo>&#x22C5;</mo>\n",
    "  <munder>\n",
    "    <mo data-mjx-texclass=\"OP\" movablelimits=\"true\">max</mo>\n",
    "    <mrow data-mjx-texclass=\"ORD\">\n",
    "      <msup>\n",
    "        <mi>d</mi>\n",
    "        <mo data-mjx-alternate=\"1\">&#x2032;</mo>\n",
    "      </msup>\n",
    "      <mo>&#x2208;</mo>\n",
    "      <msup>\n",
    "        <mi>D</mi>\n",
    "        <mo data-mjx-alternate=\"1\">&#x2032;</mo>\n",
    "      </msup>\n",
    "    </mrow>\n",
    "  </munder>\n",
    "  <mtext>Sim</mtext>\n",
    "  <mo stretchy=\"false\">(</mo>\n",
    "  <mi>d</mi>\n",
    "  <mo>,</mo>\n",
    "  <msup>\n",
    "    <mi>d</mi>\n",
    "    <mo data-mjx-alternate=\"1\">&#x2032;</mo>\n",
    "  </msup>\n",
    "  <mo stretchy=\"false\">)</mo>\n",
    "</math>\n",
    "\n",
    "- <math xmlns=\"http://www.w3.org/1998/Math/MathML\">\n",
    "  <mo stretchy=\"false\">(</mo>\n",
    "  <mtext>Sim</mtext>\n",
    "  <mo stretchy=\"false\">(</mo>\n",
    "  <mi>d</mi>\n",
    "  <mo>,</mo>\n",
    "  <mi>Q</mi>\n",
    "  <mo stretchy=\"false\">)</mo>\n",
    "  <mo stretchy=\"false\">)</mo>\n",
    "</math>\n",
    "는 문서(d)와 쿼리 (Q) 사이의 유사성\n",
    "\n",
    "- <math xmlns=\"http://www.w3.org/1998/Math/MathML\">\n",
    "  <mo stretchy=\"false\">(</mo>\n",
    "  <munder>\n",
    "    <mo data-mjx-texclass=\"OP\" movablelimits=\"true\">max</mo>\n",
    "    <mrow data-mjx-texclass=\"ORD\">\n",
    "      <msup>\n",
    "        <mi>d</mi>\n",
    "        <mo data-mjx-alternate=\"1\">&#x2032;</mo>\n",
    "      </msup>\n",
    "      <mo>&#x2208;</mo>\n",
    "      <msup>\n",
    "        <mi>D</mi>\n",
    "        <mo data-mjx-alternate=\"1\">&#x2032;</mo>\n",
    "      </msup>\n",
    "    </mrow>\n",
    "  </munder>\n",
    "  <mtext>Sim</mtext>\n",
    "  <mo stretchy=\"false\">(</mo>\n",
    "  <mi>d</mi>\n",
    "  <mo>,</mo>\n",
    "  <msup>\n",
    "    <mi>d</mi>\n",
    "    <mo data-mjx-alternate=\"1\">&#x2032;</mo>\n",
    "  </msup>\n",
    "  <mo stretchy=\"false\">)</mo>\n",
    "  <mo stretchy=\"false\">)</mo>\n",
    "</math>는 문서 (d)와 이미 선택된 문서 집합(D') 중 가장 유사한 문서와의 유사성\n",
    "\n",
    "-<math xmlns=\"http://www.w3.org/1998/Math/MathML\">\n",
    "  <mo stretchy=\"false\">(</mo>\n",
    "  <mi>&#x3BB;</mi>\n",
    "  <mo stretchy=\"false\">)</mo>\n",
    "</math> 는 유사성과 다양성의 상대적 중요도를 조절하는 매개변수 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\langchain\\venv\\lib\\site-packages\\langchain_community\\document_loaders\\parsers\\pdf.py:322: UserWarning: Warning: Empty content on page 78 of document 300720_한일시멘트_2023.pdf\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "229"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "loader = PyMuPDFLoader('323410_카카오뱅크_2023.pdf')\n",
    "data = loader.load()\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200,\n",
    "    encoding_name='cl100k_base'\n",
    ")\n",
    "documents = text_splitter.split_documents(data)\n",
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.chroma.Chroma at 0x2a650607280>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "embeddings_model = HuggingFaceEmbeddings(\n",
    "    model_name='BAAI/bge-m3',\n",
    "    model_kwargs={'device':'cuda'},\n",
    "    encode_kwargs={'normalize_embeddings':True},\n",
    ")\n",
    "\n",
    "db2 = Chroma.from_documents(\n",
    "    documents,\n",
    "    embeddings_model,\n",
    "    collection_name='esg',\n",
    "     persist_directory = './db/chromadb',\n",
    "    collection_metadata = {'hnsw:space': 'cosine'},\n",
    ")\n",
    "\n",
    "db2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 일반적인 유사도 기반 검색:  \n",
    "- 쿼리 '카카오뱅크의 환경목표와 세부추진내용을 알려줘?'를 사용하여 db2에서 유사성 검색을 수행   \n",
    "- len(docs)는 반환된 문서의 총 수를 출력   \n",
    "- docs[0].page_content는 검색 결과 중 가장 유사한 문서의 내용을 출력   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '카카오뱅크의 환경목표와 세부추진내용을 알려줘?'\n",
    "docs = db2.similarity_search(query)\n",
    "print(len(docs))\n",
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(docs[-1].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. MMR 검색:   \n",
    "동일한 쿼리를 사용하여 MMR 검색을 수행   \n",
    "len(mmr_docs)는 MMR 검색으로 선택된 문서의 총 수, 여기서는 4개를 출력   \n",
    "mmr_docs[0].page_content는 MMR 검색 결과 중 가장 높은 순위의 문서의 내용을 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mmr_docs = db2.max_marginal_relevance_search(query, k=4, fetch_k=10)\n",
    "print(len(mmr_docs))\n",
    "print(mmr_docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mmr_docs[-1].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "\n",
    "documents = [\n",
    "    Document(\n",
    "        page_content=\"LangChain은 대규모 언어 모델(LLM)을 사용하는 애플리케이션을 개발하기 위한 프레임워크입니다.\",\n",
    "        metadata={\n",
    "            \"title\": \"LangChain 소개\",\n",
    "            \"author\": \"AI 개발자\",\n",
    "            \"url\": \"http://example.com/langchain-intro\"\n",
    "        }\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"벡터 데이터베이스는 고차원 벡터를 효율적으로 저장하고 검색하는 데 특화된 데이터베이스 시스템입니다.\",\n",
    "        metadata={\n",
    "            \"title\": \"벡터 데이터베이스 개요\",\n",
    "            \"author\": \"데이터 과학자\",\n",
    "            \"url\": \"http://example.com/vector-db-overview\"\n",
    "        }\n",
    "    )\n",
    "    # 추가 문서들...\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "embedding_model = HuggingFaceEmbeddings(\n",
    "    model_name='BAAI/bge-m3',\n",
    "    model_kwargs={'device':'cuda'},\n",
    "    encode_kwargs={'normalize_embeddings':True},\n",
    ")\n",
    "\n",
    "\n",
    "# Chroma 벡터 스토어에 문서와 메타데이터 저장\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=documents,\n",
    "    embedding=embedding_model,\n",
    "    persist_directory=\"./chroma_db\"  # 벡터 스토어를 디스크에 저장\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "내용: LangChain은 대규모 언어 모델(LLM)을 사용하는 애플리케이션을 개발하기 위한 프레임워크입니다.\n",
      "제목: LangChain 소개\n",
      "저자: AI 개발자\n",
      "URL: http://example.com/langchain-intro\n",
      "---\n",
      "내용: 벡터 데이터베이스는 고차원 벡터를 효율적으로 저장하고 검색하는 데 특화된 데이터베이스 시스템입니다.\n",
      "제목: 벡터 데이터베이스 개요\n",
      "저자: 데이터 과학자\n",
      "URL: http://example.com/vector-db-overview\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "query = \"LangChain이란 무엇인가요?\"\n",
    "results = vectorstore.similarity_search(query, k=2)\n",
    "\n",
    "for doc in results:\n",
    "    print(f\"내용: {doc.page_content}\")\n",
    "    print(f\"제목: {doc.metadata['title']}\")\n",
    "    print(f\"저자: {doc.metadata['author']}\")\n",
    "    print(f\"URL: {doc.metadata['url']}\")\n",
    "    print(\"---\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-5-2. FAISS\n",
    "FAISS(Facebook AI Similarity Search)는 Facebook AI Research에 의해 개발된 라이브러리로, 대규모 벡터 데이터셋에서 유사도 검색을 빠르고 효율적으로 수행할 수 있게 해줌   \n",
    "FAISS는 특히 벡터의 압축된 표현을 사용하여 메모리 사용량을 최소화하면서도 검색 속도를 극대화하는 특징 \n",
    "\n",
    "** 1. 유사도 기반 검색(Similarity search)(Copy) **\n",
    "- distance_strategy는 벡터 간 거리(또는 유사도)를 측정하는 방법을 결정\n",
    "- FAISS.from_documents 메서드를 사용하여 문서 객체를 임베딩 벡터로 변환하여 벡터 저장소에 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.faiss.FAISS at 0x2a7e7cebc10>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.vectorstores.utils import DistanceStrategy\n",
    "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
    "\n",
    "embeddings_model = HuggingFaceEmbeddings(\n",
    "    model_name='jhgan/ko-sbert-nli',\n",
    "    model_kwargs={'device':'cpu'},\n",
    "    encode_kwargs={'normalize_embeddings':True},\n",
    ")\n",
    "\n",
    "\n",
    "vectorstore = FAISS.from_documents(documents,\n",
    "                                   embedding = embeddings_model,\n",
    "                                   distance_strategy = DistanceStrategy.COSINE\n",
    "                                  )\n",
    "vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<DistanceStrategy.COSINE: 'COSINE'>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorstore.distance_strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "vectorstore.similarity_search 메서드는 주어진 쿼리 문자열에 대해 벡터 스토어 내의 문서들 중에서 가장 유사한 문서들을 찾아내는 작업을 다음 단계에 따라 수행\n",
    "\n",
    "1. 쿼리 인코딩: 주어진 쿼리 문자열을 벡터로 변환. 이 과정은 vectorstore를 생성할 때 사용된 임베딩 모델(embeddings_model)을 사용하여 수행\n",
    "2. 유사도 검색: 변환된 쿼리 벡터와 벡터 스토어에 저장된 문서 벡터들 간의 유사도를 계산하여, 가장 유사한 문서들을 찾아냄. 이 때, 유사도 계산 방법은 vectorstore 생성 시 지정된 distance_strategy에 따라 결정.\n",
    "3. 결과 반환: 검색 결과로 얻어진 문서들을 유사도 순으로 정렬하여 반환.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "LangChain은 대규모 언어 모델(LLM)을 사용하는 애플리케이션을 개발하기 위한 프레임워크입니다.\n"
     ]
    }
   ],
   "source": [
    "query = '카카오뱅크가 중대성 평가를 통해 도출한 6가지 중대 주제는 무엇인가?'\n",
    "docs = vectorstore.similarity_search(query)\n",
    "print(len(docs))\n",
    "print(docs[0].page_content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** 2-5-2-2. MMR (Maximum marginal relevance search) (Copy) **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mmr_docs = vectorstore.max_marginal_relevance_search(query, k=4, fetch_k=10)\n",
    "print(len(mmr_docs))\n",
    "print(mmr_docs[0].page_content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** 3. FAISS DB를 로컬에 저장하기 **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 벡터 스토어 저장하기\n",
    "vectorstore.save_local('./db/faiss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The de-serialization relies loading a pickle file. Pickle files can be modified to deliver a malicious payload that results in execution of arbitrary code on your machine.You will need to set `allow_dangerous_deserialization` to `True` to enable deserialization. If you do this, make sure that you trust the source of the data. For example, if you are loading a file that you created, and know that no one else has modified the file, then this is safe to do. Do not set this to `True` if you are loading a file from an untrusted source (e.g., some random site on the internet.).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# 벡터 스토어 불러오기\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m db3 \u001b[38;5;241m=\u001b[39m \u001b[43mFAISS\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_local\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./db/faiss\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding_model\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\langchain\\venv\\lib\\site-packages\\langchain_community\\vectorstores\\faiss.py:1190\u001b[0m, in \u001b[0;36mFAISS.load_local\u001b[1;34m(cls, folder_path, embeddings, index_name, allow_dangerous_deserialization, **kwargs)\u001b[0m\n\u001b[0;32m   1176\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Load FAISS index, docstore, and index_to_docstore_id from disk.\u001b[39;00m\n\u001b[0;32m   1177\u001b[0m \n\u001b[0;32m   1178\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1187\u001b[0m \u001b[38;5;124;03m        arbitrary code on your machine.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1189\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m allow_dangerous_deserialization:\n\u001b[1;32m-> 1190\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1191\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe de-serialization relies loading a pickle file. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1192\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPickle files can be modified to deliver a malicious payload that \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1193\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresults in execution of arbitrary code on your machine.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1194\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou will need to set `allow_dangerous_deserialization` to `True` to \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1195\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menable deserialization. If you do this, make sure that you \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1196\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrust the source of the data. For example, if you are loading a \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1197\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile that you created, and know that no one else has modified the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1198\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile, then this is safe to do. Do not set this to `True` if you are \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1199\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloading a file from an untrusted source (e.g., some random site on \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1200\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthe internet.).\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1201\u001b[0m     )\n\u001b[0;32m   1202\u001b[0m path \u001b[38;5;241m=\u001b[39m Path(folder_path)\n\u001b[0;32m   1203\u001b[0m \u001b[38;5;66;03m# load index separately since it is not picklable\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: The de-serialization relies loading a pickle file. Pickle files can be modified to deliver a malicious payload that results in execution of arbitrary code on your machine.You will need to set `allow_dangerous_deserialization` to `True` to enable deserialization. If you do this, make sure that you trust the source of the data. For example, if you are loading a file that you created, and know that no one else has modified the file, then this is safe to do. Do not set this to `True` if you are loading a file from an untrusted source (e.g., some random site on the internet.)."
     ]
    }
   ],
   "source": [
    "# 벡터 스토어 불러오기\n",
    "db3 = FAISS.load_local('./db/faiss', embedding_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
