{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Public\\Documents\\ESTsoft\\CreatorTemp\\ipykernel_9544\\2891556783.py:2: LangChainDeprecationWarning: The class `ChatOllama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import ChatOllama``.\n",
      "  llm = ChatOllama(model=\"Llama-3-Open-Ko-8B:latest\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='지구는 태양을 중심으로 공전하는 행성입니다. 지구는 태양과 약 1억 5천만 킬로미터 떨어져 있습니다. 지구는 태양을 한 바퀴 도는데 약 365일이 걸립니다.', additional_kwargs={}, response_metadata={'model': 'Llama-3-Open-Ko-8B:latest', 'created_at': '2025-01-07T06:33:40.25509Z', 'message': {'role': 'assistant', 'content': ''}, 'done_reason': 'stop', 'done': True, 'total_duration': 18105901300, 'load_duration': 16145700, 'prompt_eval_count': 60, 'prompt_eval_duration': 2057000000, 'eval_count': 60, 'eval_duration': 16030000000}, id='run-2c697ec4-c7be-4d41-b5ed-f90071b34a47-0')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.chat_models import ChatOllama\n",
    "llm = ChatOllama(model=\"Llama-3-Open-Ko-8B:latest\")\n",
    "llm.invoke(\"지구의 자전 주기는?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='천문학 분야의 전문가로서 행동해줘.<Qusetion>:{input}'), additional_kwargs={})])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"천문학 분야의 전문가로서 행동해줘.<Qusetion>:{input}\")\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='지구는 태양계에서 가장 큰 행성입니다. 지구의 자전 주기는 약 23시간 56분 4초이며, 이는 태양과 달 사이의 거리보다 약 30만 배 더 짧습니다.', additional_kwargs={}, response_metadata={'model': 'Llama-3-Open-Ko-8B:latest', 'created_at': '2025-01-07T06:33:58.336611Z', 'message': {'role': 'assistant', 'content': ''}, 'done_reason': 'stop', 'done': True, 'total_duration': 15893501000, 'load_duration': 16186800, 'prompt_eval_count': 77, 'prompt_eval_duration': 1305000000, 'eval_count': 54, 'eval_duration': 14570000000}, id='run-f7539ddc-d331-4576-b7d2-94944f955e8c-0')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = prompt | llm\n",
    "\n",
    "chain.invoke({\"input\": \"지구의 자전 주기는?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "프롬프트, LLM, 문자열 출력 파서(StrOutputParser)를 연결하여 체인을 만들 수 있음\n",
    "\n",
    "StrOutputParser는 모델의 출력을 문자열 형태로 파싱하여 최종 결과를 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'지구는 태양계에서 가장 큰 행성입니다. 지구의 자전 주기는 약 23시간 56분 4초이며, 이는 태양과 달 사이의 거리보다 약 30만 배 더 짧습니다.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"천문학 분야의 전문가로서 행동해줘.<Qusetion>:{input}\")\n",
    "llm = ChatOllama(model=\"Llama-3-Open-Ko-8B:latest\")\n",
    "output_parset = StrOutputParser()\n",
    "\n",
    "chain = prompt | llm | output_parset\n",
    "\n",
    "chain.invoke({\"input\": \"지구의 자전 주기는?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-2-2 멀티 체인(Multi-Chain)\n",
    "\n",
    "- 여러 개의 체인을 연결하거나 복합적으로 작용하는 것은 멀티 체인(Multi-Chain) 구조를 통해 이루어짐\n",
    "\n",
    "1. 순차적인 체인 연결\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The word \"미래\" (mi-re) translates to \"future\" in English.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt1 = ChatPromptTemplate.from_template(\"translates {korean_word} to English.\")\n",
    "prompt2 = ChatPromptTemplate.from_template(\n",
    "    \"explain {english_word} using oxford dictionary to me in Korean.\"\n",
    ")\n",
    "\n",
    "llm = ChatOllama(model=\"llama3:latest\")\n",
    "chain1 = prompt1 | llm | StrOutputParser()\n",
    "\n",
    "chain1.invoke({\"korean_word\":\"미래\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The word \"\" (mirae) is a fascinating one!\\n\\nAccording to the Oxford Dictionary, you\\'re correct that \"\" can be translated to English as:\\n\\n* Future\\n* Tomorrow\\n* Ahead\\n* Next (as in, the next step or action)\\n\\nHowever, as you mentioned, the connotations of \"\" are more nuanced in Korean culture. It often carries optimistic and hopeful undertones, conveying a sense of promise and possibility.\\n\\nIn Korean, \"\" is often used to express aspirations and goals for one\\'s life, such as \"my future career\" or \"my future marriage\". This usage implies a sense of hope and anticipation for the future.\\n\\nMoreover, when Koreans say \"a brighter future lies ahead,\" it\\'s not just about looking forward to what\\'s coming next; it\\'s about holding onto hope that things will improve. The word \"\" seems to carry a message of encouragement and positivity, urging people to strive for better times.\\n\\nIn contrast, when we use the English translation \"future\" in isolation, it might simply mean the time period after now, without necessarily conveying optimism or hope. This highlights the cultural differences between Korean and English speakers when it comes to using this word.\\n\\nThank you for pointing out the nuances of \"\"! It\\'s an important reminder that words can have different connotations and implications depending on the cultural context in which they\\'re used.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain2 =(\n",
    "    {\"english_word\":chain1}\n",
    "    | prompt2\n",
    "    |llm\n",
    "    |StrOutputParser()\n",
    ")\n",
    "\n",
    "chain2.invoke({\"korean_word\":\"미래\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-2-3. 체인을 실행하는 방법\n",
    "\"Runnable\" 프로토콜\n",
    "- invoke: 주어진 입력에 대해 체인을 호출하고, 결과를 반환. 이 메소드는 단일 입력에 대해 동기적으로 작동.\n",
    "- batch: 입력 리스트에 대해 체인을 호출하고, 각 입력에 대한 결과를 리스트로 반환. 이 메소드는 여러 입력에 대해 동기적으로 작동하며, 효율적인 배치 처리를 가능하게 함.\n",
    "- stream: 입력에 대해 체인을 호출하고, 결과의 조각들을 스트리밍. 이는 대용량 데이터 처리나 실시간 데이터 처리에 유용.\n",
    "- 비동기 버전: ainvoke, abatch, astream 등의 메소드는 각각의 동기 버전에 대한 비동기 실행을 지원. 이를 통해 비동기 프로그래밍 패러다임을 사용하여 더 높은 처리 성능과 효율을 달성할 수 있음.\n",
    "\n",
    "\n",
    "LangChain을 사용하여 커스텀 체인을 생성하는 과정\n",
    "1. 필요한 컴포넌트를 정의하고, 각각 \"Runnable\" 인터페이스를 구현\n",
    "2. 컴포넌트들을 조합하여 사용자 정의 체인을 생성\n",
    "3. 생성된 체인을 사용하여 데이터 처리 작업을 수행. invoke, batch, stream 메소드를 사용하여 원하는 방식으로 데이터를 처리할 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "invoke 결과: 지구는 태양 주위를 공전하면서도 자전을 합니다. 지구의 자전 속도는 초당 1,670km이며, 하루에 한 바퀴를 도는데 약 24시간이 걸립니다. 지구가 자전하는 이유는 지구 내부에서 발생하는 열 때문입니다.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "\n",
    "# 컴포넌트 정의\n",
    "prompt = ChatPromptTemplate.from_template(\"지구과학에서 {topic}에 대해 간단히 설명해주세요.\")\n",
    "llm = ChatOllama(model=\"Llama-3-Open-Ko-8B:latest\")\n",
    "output_parset = StrOutputParser()\n",
    "\n",
    "chain = prompt | llm | output_parset\n",
    "\n",
    "result = chain.invoke({\"topic\": \"지구 자전\"})\n",
    "print(\"invoke 결과:\", result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "지구 공전 설명: 지구는 태양 주위를 1년 동안 한 바퀴 도는데요. 지구의 자전축은 약 23.5도 기울어져 ...\n",
      "화산 활동 설명: 화산은 지각의 일부가 뜨거워져서 분출하는 현상입니다. 지각이 뜨겁게 되면, 마그마가 형성되...\n",
      "대륙 이동 설명: 대륙 이동은 지각의 움직임으로 인한 것으로, 약 2억 년 전부터 시작되어 현재까지 계속되고...\n"
     ]
    }
   ],
   "source": [
    "#batch 메소드 사용\n",
    "topics = [\"지구 공전\", \"화산 활동\", \"대륙 이동\"]\n",
    "results = chain.batch([{\"topic\": t} for t in topics])\n",
    "for topic, result in zip(topics, results):\n",
    "    print(f\"{topic} 설명: {result[:50]}...\")  # 결과의 처음 50자만 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stream 결과:\n",
      "지진은 지각의 움직임으로 인한 진동을 말합니다. 지진이 발생하면 지표면이 흔들리고 건물이나 도로가 파괴될 수 있습니다. 지진은 크게 자연지진과 인공지진으로 나뉩니다.\n"
     ]
    }
   ],
   "source": [
    "# stream 메소드 사용\n",
    "stream = chain.stream({\"topic\": \"지진\"})\n",
    "print(\"stream 결과:\")\n",
    "for chunk in stream:\n",
    "    print(chunk, end=\"\", flush=True)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ainvoke 결과: 해류는 바다의 물이 바람이나 달의 인력 등으로 생기는 힘을 받아서 이동하는 흐름입니다. 해 ...\n"
     ]
    }
   ],
   "source": [
    "import nest_asyncio\n",
    "import asyncio\n",
    "\n",
    "# nest_asyncio 적용 (구글 코랩 등 주피터 노트북에서 실행 필요)\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# 비동기 메소드 사용 (async/await 구문 필요)\n",
    "async def run_async():\n",
    "    result = await chain.ainvoke({\"topic\": \"해류\"})\n",
    "    print(\"ainvoke 결과:\", result[:50], \"...\")\n",
    "\n",
    "asyncio.run(run_async())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1-3. 프롬프트(Prompt)\n",
    "프롬프트는 사용자와 언어 모델 간의 대화에서 질문이나 요청의 형태로 제시되는 입력문\n",
    "\n",
    "## 1-3-1. 프롬프트 작성 원칙\n",
    "1. 명확성과 구체성\n",
    "2. 배경 정보를 포함\n",
    "3. 간결함\n",
    "4. 열린 질문 사용\n",
    "5. 명확한 목표 설정\n",
    "6. 언어와 문체\n",
    "\n",
    "## 1-3-2. 프롬프트 템플릿(PromptTemplate)\n",
    "- PromptTemplate은 단일 문장 또는 간단한 명령을 입력하여 단일 문장 또는 간단한 응답을 생성하는 데 사용되는 프롬프트를 구성할 수 있는 문자열 템플릿\n",
    "1. 구성요소\n",
    "지시 : 언어 모델에게 어떤 작업을 수행하도록 요청하는 구체적인 지시.   \n",
    "예시 : 요청된 작업을 수행하는 방법에 대한 하나 이상의 예시.   \n",
    "맥락 : 특정 작업을 수행하기 위한 추가적인 맥락   \n",
    "질문 : 어떤 답변을 요구하는 구체적인 질문.   \n",
    "\n",
    "2. 문자열 탬플릿\n",
    "langchain_core.prompts 모듈의 PromptTemplate 클래스를 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'안녕하세요, 제 이름은 홍길동입니다. 30살입니다.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "template_text = \"안녕하세요, 제 이름은 {name}입니다. {age}살입니다.\"\n",
    "prompt_template = PromptTemplate.from_template(template_text)\n",
    "filled_prompt = prompt_template.format(name=\"홍길동\", age=30)\n",
    "filled_prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. 프롬프트 템플릿 간의 결합\n",
    "- PromptTemplate 클래스는 문자열을 기반으로 프롬프트 템플릿을 생성하고, + 연산자를 사용하여 직접 결합하는 동작을 지원   \n",
    "    - 인스턴스 간의 직접적인 결합뿐만 아니라, 이들 인스턴스와 문자열로 이루어진 템플릿을 결합하여 새로운 PromptTemplate 인스턴스를 생성하는 것도 가능   \n",
    "\n",
    "ex) \n",
    "문자열 + 문자열   \n",
    "PromptTemplate + PromptTemplate   \n",
    "PromptTemplate + 문자열   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['age', 'language', 'name'], input_types={}, partial_variables={}, template='안녕하세요, 제 이름은 {name}입니다. {age}살입니다.\\n\\n아버지를 아버지라 부를 수 없습니다.\\n\\n{language}로 번역해주세요.')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 문자열 템플릿 결합 (PromptTemplate + PromptTemplate + 문자열)\n",
    "combined_prompt = (\n",
    "              prompt_template\n",
    "              + PromptTemplate.from_template(\"\\n\\n아버지를 아버지라 부를 수 없습니다.\")\n",
    "              + \"\\n\\n{language}로 번역해주세요.\"\n",
    ")\n",
    "\n",
    "combined_prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'안녕하세요, 제 이름은 홍길동입니다. 30살입니다.\\n\\n아버지를 아버지라 부를 수 없습니다.\\n\\n영어로 번역해주세요.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_prompt.format(name=\"홍길동\", age=30, language=\"영어\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ChatOpenAI 인스턴스를 생성하여 프롬프트 텍스트를 전달하고, 모델의 출력을 StrOutputParser를 통해 문자열로 변환하는 LLM 체인을 구성   \n",
    "invoke 메소드를 사용하여 파이프라인을 실행하고, 최종적으로 문자열 출력을 얻음   \n",
    "모델의 응답은 프롬프트에 주어진 문장을 영어로 번역한 텍스트가 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hello, my name is Hong Gil-dong. I am 30 years old.\\nI can't call my father a father.\\nPlease translate it into English.\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "chain = combined_prompt | llm | StrOutputParser()\n",
    "chain.invoke({\"age\":30, \"language\":\"영어\", \"name\":\"홍길동\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1-3-3. 챗 프롬프트 템플릿 (ChatPromptTemplate)\n",
    "- 대화형 상황에서 여러 메시지 입력을 기반으로 단일 메시지 응답을 생성하는 데 사용\n",
    "- 대화형 모델이나 챗봇 개발에 주로 사용\n",
    "- 입력은 여러 메시지를 원소로 갖는 리스트로 구성되며, 각 메시지는 역할(role)과 내용(content)으로 구성   \n",
    "\n",
    "1. Message 유형\n",
    "SystemMessage: 시스템의 기능을 설명   \n",
    "HumanMessage: 사용자의 질문을 나타냄   \n",
    "AIMessage: AI 모델의 응답을 제공   \n",
    "FunctionMessage: 특정 함수 호출의 결과를 나타냄   \n",
    "ToolMessage: 도구 호출의 결과를 나타냄   \n",
    "\n",
    "2. 2-튜플 형태의 메시지 리스트\n",
    "ChatPromptTemplate.from_messages 메서드 :  2-튜플 형태의 메시지 리스트를 입력 받아, 각 메시지의 역할(type)과 내용(content)을 기반으로 프롬프트를 구성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='이 시스템은 천문학 질문에 답변할 수 있습니다.', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='태양계에서 가장 큰 행성은 무엇인가요?', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2-튜플 형태의 메시지 목록으로 프롬프트 생성 (type, content)\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"이 시스템은 천문학 질문에 답변할 수 있습니다.\"),\n",
    "    (\"user\", \"{user_input}\"),\n",
    "])\n",
    "\n",
    "messages = chat_prompt.format_messages(user_input=\"태양계에서 가장 큰 행성은 무엇인가요?\")\n",
    "messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "chat_prompt, llm, StrOutputParser()를 순차적인 파이프라인으로 연결하여 구성된 chain을 사용   \n",
    "invoke 메소드를 호출하면 사용자 입력을 받아 언어 모델에 전달하고, 모델의 응답을 처리하여 최종 문자열 결과를 반환하는 과정을 자동화하여 수행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'지구입니다. 지구는 태양계에서 네 번째로 크고, 목성보다 약 10배 더 무겁습니다.'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "chain = chat_prompt | llm | StrOutputParser()\n",
    "\n",
    "chain.invoke({\"user_input\": \"태양계에서 가장 큰 행성은 무엇인가요?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. MessagePromptTemplate 사용\n",
    "ChatPromptTemplate.from_messages 메소드를 통해 시스템 메시지와 사용자 메시지 템플릿을 포함하는 챗 프롬프트를 구성 -> 이후, chat_prompt.format_messages 메서드를 사용하여 사용자의 질문을 포함한 메시지 리스트를 동적으로 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='이 시스템은 천문학 질문에 답변할 수 있습니다.', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='태양계에서 가장 큰 행성은 무엇인가요?', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# MessagePromptTemplate 활용\n",
    "\n",
    "from langchain_core.prompts import SystemMessagePromptTemplate,  HumanMessagePromptTemplate\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        SystemMessagePromptTemplate.from_template(\"이 시스템은 천문학 질문에 답변할 수 있습니다.\"),\n",
    "        HumanMessagePromptTemplate.from_template(\"{user_input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "messages = chat_prompt.format_messages(user_input=\"태양계에서 가장 큰 행성은 무엇인가요?\")\n",
    "messages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'지구입니다. 지구는 태양계에서 네 번째로 크고, 목성보다 약 10배 더 무겁습니다.'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = chat_prompt | llm | StrOutputParser()\n",
    "\n",
    "chain.invoke({\"user_input\": \"태양계에서 가장 큰 행성은 무엇인가요?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1-3-4. Few-shot-Prompt\n",
    "- Few-shot 학습은 언어 모델에 몇 가지 예시를 제공하여 특정 작업을 수행하도록 유도하는 기법\n",
    "\n",
    "1. Few-shot 예제 포맷터 생성\n",
    "- PromptTemplate 은 질문과 답변을 포함하는 간단한 구조"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "example_prompt = PromptTemplate.from_template(\"질문: {question}\\n{answer}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = [\n",
    "    {\n",
    "        \"question\": \"지구의 대기 중 가장 많은 비율을 차지하는 기체는 무엇인가요?\",\n",
    "        \"answer\": \"지구 대기의 약 78%를 차지하는 질소입니다.\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"광합성에 필요한 주요 요소들은 무엇인가요?\",\n",
    "        \"answer\": \"광합성에 필요한 주요 요소는 빛, 이산화탄소, 물입니다.\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"피타고라스 정리를 설명해주세요.\",\n",
    "        \"answer\": \"피타고라스 정리는 직각삼각형에서 빗변의 제곱이 다른 두 변의 제곱의 합과 같다는 것입니다.\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"지구의 자전 주기는 얼마인가요?\",\n",
    "        \"answer\": \"지구의 자전 주기는 약 24시간(정확히는 23시간 56분 4초)입니다.\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"DNA의 기본 구조를 간단히 설명해주세요.\",\n",
    "        \"answer\": \"DNA는 두 개의 폴리뉴클레오티드 사슬이 이중 나선 구조를 이루고 있습니다.\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"원주율(π)의 정의는 무엇인가요?\",\n",
    "        \"answer\": \"원주율(π)은 원의 지름에 대한 원의 둘레의 비율입니다.\"\n",
    "    }\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. FewShotPromptTemplate 생성\n",
    "- FewShotPromptTemplate 은 예제들을 결합하고 새로운 입력을 추가하여 최종 프롬프트를 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "질문: 지구의 대기 중 가장 많은 비율을 차지하는 기체는 무엇인가요?\n",
      "지구 대기의 약 78%를 차지하는 질소입니다.\n",
      "\n",
      "질문: 광합성에 필요한 주요 요소들은 무엇인가요?\n",
      "광합성에 필요한 주요 요소는 빛, 이산화탄소, 물입니다.\n",
      "\n",
      "질문: 피타고라스 정리를 설명해주세요.\n",
      "피타고라스 정리는 직각삼각형에서 빗변의 제곱이 다른 두 변의 제곱의 합과 같다는 것입니다.\n",
      "\n",
      "질문: 지구의 자전 주기는 얼마인가요?\n",
      "지구의 자전 주기는 약 24시간(정확히는 23시간 56분 4초)입니다.\n",
      "\n",
      "질문: DNA의 기본 구조를 간단히 설명해주세요.\n",
      "DNA는 두 개의 폴리뉴클레오티드 사슬이 이중 나선 구조를 이루고 있습니다.\n",
      "\n",
      "질문: 원주율(π)의 정의는 무엇인가요?\n",
      "원주율(π)은 원의 지름에 대한 원의 둘레의 비율입니다.\n",
      "\n",
      "질문: 화성의 표면이 붉은 이유는 무엇인가요?\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import FewShotPromptTemplate\n",
    "\n",
    "# FewShotPromptTemplate을 생성합니다.\n",
    "prompt = FewShotPromptTemplate(\n",
    "    examples=examples,              # 사용할 예제들\n",
    "    example_prompt=example_prompt,  # 예제 포맷팅에 사용할 템플릿\n",
    "    suffix=\"질문: {input}\",          # 예제 뒤에 추가될 접미사\n",
    "    input_variables=[\"input\"],      # 입력 변수 지정\n",
    ")\n",
    "\n",
    "# 새로운 질문에 대한 프롬프트를 생성하고 출력합니다.\n",
    "print(prompt.invoke({\"input\": \"화성의 표면이 붉은 이유는 무엇인가요?\"}).to_string())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. 예제 선택기 사용하기\n",
    "-  분석: SemanticSimilarityExampleSelector는 입력 질문과 가장 유사한 예제를 선택"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Public\\Documents\\ESTsoft\\CreatorTemp\\ipykernel_9544\\3304479034.py:6: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(\n",
      "c:\\langchain\\venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력과 가장 유사한 예제: 화성의 표면이 붉은 이유는 무엇인가요?\n",
      "\n",
      "\n",
      "answer: 광합성에 필요한 주요 요소는 빛, 이산화탄소, 물입니다.\n",
      "question: 광합성에 필요한 주요 요소들은 무엇인가요?\n"
     ]
    }
   ],
   "source": [
    "from langchain_chroma import Chroma\n",
    "from langchain_core.example_selectors import SemanticSimilarityExampleSelector\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "# HuggingFace 임베딩 모델 초기화\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name='BAAI/bge-m3',\n",
    "    model_kwargs={'device': 'cuda'},  # GPU 사용\n",
    "    encode_kwargs={'normalize_embeddings': True}  # 임베딩 정규화\n",
    ")\n",
    "\n",
    "# SemanticSimilarityExampleSelector를 초기화\n",
    "example_selector = SemanticSimilarityExampleSelector.from_examples(\n",
    "    examples,  # 사용할 예제들\n",
    "    embeddings,  # HuggingFace 임베딩 모델 사용\n",
    "    Chroma,  # 벡터 저장소\n",
    "    k=1  # 선택할 예제 수\n",
    ")\n",
    "\n",
    "# 새로운 질문에 대해 가장 유사한 예제를 선택\n",
    "question = \"화성의 표면이 붉은 이유는 무엇인가요?\"\n",
    "selected_examples = example_selector.select_examples({\"question\": question})\n",
    "\n",
    "print(f\"입력과 가장 유사한 예제: {question}\")\n",
    "for example in selected_examples:\n",
    "    print(\"\\n\")\n",
    "    for k, v in example.items():\n",
    "        print(f\"{k}: {v}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "채팅 모델에서 Few-shot 예제 사용하기\n",
    "1. 고정 예제 사용하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24시간입니다.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, FewShotChatMessagePromptTemplate\n",
    "\n",
    "# 예제 정의\n",
    "examples = [\n",
    "    {\"input\": \"지구의 대기 중 가장 많은 비율을 차지하는 기체는 무엇인가요?\", \"output\": \"질소입니다.\"},\n",
    "    {\"input\": \"광합성에 필요한 주요 요소들은 무엇인가요?\", \"output\": \"빛, 이산화탄소, 물입니다.\"},\n",
    "]\n",
    "\n",
    "# 예제 프롬프트 템플릿 정의\n",
    "example_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"human\", \"{input}\"),\n",
    "        (\"ai\", \"{output}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Few-shot 프롬프트 템플릿 생성\n",
    "few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
    "    example_prompt=example_prompt,\n",
    "    examples=examples,\n",
    ")\n",
    "\n",
    "# 최종 프롬프트 템플릿 생성\n",
    "final_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"당신은 과학과 수학에 대해 잘 아는 교육자입니다.\"),\n",
    "        few_shot_prompt,\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "\n",
    "model = ChatOllama(model=\"Llama-3-Open-Ko-8B:latest\")\n",
    "chain = final_prompt | model\n",
    "\n",
    "result = chain.invoke({\"input\": \"지구의 자전 주기는 얼마인가요?\"})\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 동적 Few-shot 프롬프팅\n",
    "- 예제 선택기(ExampleSelector)를 사용해서 입력에 따라 전체 예제 세트에서 가장 관련성 높은 예제만 선택하여 보여주는 방법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'input'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 51\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;66;03m# 모델과 체인 생성\u001b[39;00m\n\u001b[0;32m     49\u001b[0m chain \u001b[38;5;241m=\u001b[39m final_prompt \u001b[38;5;241m|\u001b[39m ChatOllama(model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLlama-3-Open-Ko-8B:latest\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 51\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mchain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m태양계에서 가장 큰 행성은 무엇인가요?\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28mprint\u001b[39m(result\u001b[38;5;241m.\u001b[39mcontent)\n",
      "File \u001b[1;32mc:\\langchain\\venv\\lib\\site-packages\\langchain_core\\runnables\\base.py:3020\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m   3018\u001b[0m context\u001b[38;5;241m.\u001b[39mrun(_set_config_context, config)\n\u001b[0;32m   3019\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m-> 3020\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mrun(step\u001b[38;5;241m.\u001b[39minvoke, \u001b[38;5;28minput\u001b[39m, config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   3021\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   3022\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mrun(step\u001b[38;5;241m.\u001b[39minvoke, \u001b[38;5;28minput\u001b[39m, config)\n",
      "File \u001b[1;32mc:\\langchain\\venv\\lib\\site-packages\\langchain_core\\prompts\\base.py:208\u001b[0m, in \u001b[0;36mBasePromptTemplate.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtags:\n\u001b[0;32m    207\u001b[0m     config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtags\n\u001b[1;32m--> 208\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_with_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    209\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_format_prompt_with_error_handling\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    210\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    211\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    212\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrun_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprompt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    213\u001b[0m \u001b[43m    \u001b[49m\u001b[43mserialized\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_serialized\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    214\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\langchain\\venv\\lib\\site-packages\\langchain_core\\runnables\\base.py:1925\u001b[0m, in \u001b[0;36mRunnable._call_with_config\u001b[1;34m(self, func, input, config, run_type, serialized, **kwargs)\u001b[0m\n\u001b[0;32m   1921\u001b[0m     context \u001b[38;5;241m=\u001b[39m copy_context()\n\u001b[0;32m   1922\u001b[0m     context\u001b[38;5;241m.\u001b[39mrun(_set_config_context, child_config)\n\u001b[0;32m   1923\u001b[0m     output \u001b[38;5;241m=\u001b[39m cast(\n\u001b[0;32m   1924\u001b[0m         Output,\n\u001b[1;32m-> 1925\u001b[0m         context\u001b[38;5;241m.\u001b[39mrun(\n\u001b[0;32m   1926\u001b[0m             call_func_with_variable_args,  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m   1927\u001b[0m             func,  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m   1928\u001b[0m             \u001b[38;5;28minput\u001b[39m,  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m   1929\u001b[0m             config,\n\u001b[0;32m   1930\u001b[0m             run_manager,\n\u001b[0;32m   1931\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   1932\u001b[0m         ),\n\u001b[0;32m   1933\u001b[0m     )\n\u001b[0;32m   1934\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1935\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[1;32mc:\\langchain\\venv\\lib\\site-packages\\langchain_core\\runnables\\config.py:396\u001b[0m, in \u001b[0;36mcall_func_with_variable_args\u001b[1;34m(func, input, config, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    394\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m accepts_run_manager(func):\n\u001b[0;32m    395\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m run_manager\n\u001b[1;32m--> 396\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\langchain\\venv\\lib\\site-packages\\langchain_core\\prompts\\base.py:183\u001b[0m, in \u001b[0;36mBasePromptTemplate._format_prompt_with_error_handling\u001b[1;34m(self, inner_input)\u001b[0m\n\u001b[0;32m    181\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_format_prompt_with_error_handling\u001b[39m(\u001b[38;5;28mself\u001b[39m, inner_input: \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m PromptValue:\n\u001b[0;32m    182\u001b[0m     _inner_input \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_input(inner_input)\n\u001b[1;32m--> 183\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformat_prompt(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m_inner_input)\n",
      "File \u001b[1;32mc:\\langchain\\venv\\lib\\site-packages\\langchain_core\\prompts\\chat.py:785\u001b[0m, in \u001b[0;36mBaseChatPromptTemplate.format_prompt\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    776\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mformat_prompt\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m PromptValue:\n\u001b[0;32m    777\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Format prompt. Should return a PromptValue.\u001b[39;00m\n\u001b[0;32m    778\u001b[0m \n\u001b[0;32m    779\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    783\u001b[0m \u001b[38;5;124;03m        PromptValue.\u001b[39;00m\n\u001b[0;32m    784\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 785\u001b[0m     messages \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformat_messages(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ChatPromptValue(messages\u001b[38;5;241m=\u001b[39mmessages)\n",
      "File \u001b[1;32mc:\\langchain\\venv\\lib\\site-packages\\langchain_core\\prompts\\chat.py:1228\u001b[0m, in \u001b[0;36mChatPromptTemplate.format_messages\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m   1224\u001b[0m     result\u001b[38;5;241m.\u001b[39mextend([message_template])\n\u001b[0;32m   1225\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[0;32m   1226\u001b[0m     message_template, (BaseMessagePromptTemplate, BaseChatPromptTemplate)\n\u001b[0;32m   1227\u001b[0m ):\n\u001b[1;32m-> 1228\u001b[0m     message \u001b[38;5;241m=\u001b[39m message_template\u001b[38;5;241m.\u001b[39mformat_messages(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1229\u001b[0m     result\u001b[38;5;241m.\u001b[39mextend(message)\n\u001b[0;32m   1230\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\langchain\\venv\\lib\\site-packages\\langchain_core\\prompts\\few_shot.py:391\u001b[0m, in \u001b[0;36mFewShotChatMessagePromptTemplate.format_messages\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    389\u001b[0m \u001b[38;5;66;03m# Get the examples to use.\u001b[39;00m\n\u001b[0;32m    390\u001b[0m examples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_examples(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m--> 391\u001b[0m examples \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    392\u001b[0m     {k: e[k] \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexample_prompt\u001b[38;5;241m.\u001b[39minput_variables} \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m examples\n\u001b[0;32m    393\u001b[0m ]\n\u001b[0;32m    394\u001b[0m \u001b[38;5;66;03m# Format the examples.\u001b[39;00m\n\u001b[0;32m    395\u001b[0m messages \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    396\u001b[0m     message\n\u001b[0;32m    397\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m example \u001b[38;5;129;01min\u001b[39;00m examples\n\u001b[0;32m    398\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m message \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexample_prompt\u001b[38;5;241m.\u001b[39mformat_messages(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mexample)\n\u001b[0;32m    399\u001b[0m ]\n",
      "File \u001b[1;32mc:\\langchain\\venv\\lib\\site-packages\\langchain_core\\prompts\\few_shot.py:392\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    389\u001b[0m \u001b[38;5;66;03m# Get the examples to use.\u001b[39;00m\n\u001b[0;32m    390\u001b[0m examples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_examples(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    391\u001b[0m examples \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m--> 392\u001b[0m     {k: e[k] \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexample_prompt\u001b[38;5;241m.\u001b[39minput_variables} \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m examples\n\u001b[0;32m    393\u001b[0m ]\n\u001b[0;32m    394\u001b[0m \u001b[38;5;66;03m# Format the examples.\u001b[39;00m\n\u001b[0;32m    395\u001b[0m messages \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    396\u001b[0m     message\n\u001b[0;32m    397\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m example \u001b[38;5;129;01min\u001b[39;00m examples\n\u001b[0;32m    398\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m message \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexample_prompt\u001b[38;5;241m.\u001b[39mformat_messages(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mexample)\n\u001b[0;32m    399\u001b[0m ]\n",
      "File \u001b[1;32mc:\\langchain\\venv\\lib\\site-packages\\langchain_core\\prompts\\few_shot.py:392\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    389\u001b[0m \u001b[38;5;66;03m# Get the examples to use.\u001b[39;00m\n\u001b[0;32m    390\u001b[0m examples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_examples(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    391\u001b[0m examples \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m--> 392\u001b[0m     {k: \u001b[43me\u001b[49m\u001b[43m[\u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexample_prompt\u001b[38;5;241m.\u001b[39minput_variables} \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m examples\n\u001b[0;32m    393\u001b[0m ]\n\u001b[0;32m    394\u001b[0m \u001b[38;5;66;03m# Format the examples.\u001b[39;00m\n\u001b[0;32m    395\u001b[0m messages \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    396\u001b[0m     message\n\u001b[0;32m    397\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m example \u001b[38;5;129;01min\u001b[39;00m examples\n\u001b[0;32m    398\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m message \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexample_prompt\u001b[38;5;241m.\u001b[39mformat_messages(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mexample)\n\u001b[0;32m    399\u001b[0m ]\n",
      "\u001b[1;31mKeyError\u001b[0m: 'input'"
     ]
    }
   ],
   "source": [
    "from langchain_chroma import Chroma\n",
    "from langchain_core.example_selectors import SemanticSimilarityExampleSelector\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "# 더 많은 예제 추가\n",
    "examples = [\n",
    "    {\"input\": \"지구의 대기 중 가장 많은 비율을 차지하는 기체는 무엇인가요?\", \"output\": \"질소입니다.\"},\n",
    "    {\"input\": \"광합성에 필요한 주요 요소들은 무엇인가요?\", \"output\": \"빛, 이산화탄소, 물입니다.\"},\n",
    "    {\"input\": \"피타고라스 정리를 설명해주세요.\", \"output\": \"직각삼각형에서 빗변의 제곱은 다른 두 변의 제곱의 합과 같습니다.\"},\n",
    "    {\"input\": \"DNA의 기본 구조를 간단히 설명해주세요.\", \"output\": \"DNA는 이중 나선 구조를 가진 핵산입니다.\"},\n",
    "    {\"input\": \"원주율(π)의 정의는 무엇인가요?\", \"output\": \"원의 둘레와 지름의 비율입니다.\"},\n",
    "]\n",
    "\n",
    "# 벡터 저장소 생성\n",
    "to_vectorize = [\" \".join(example.values()) for example in examples]\n",
    "\n",
    "# HuggingFace 임베딩 모델 초기화\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name='BAAI/bge-m3',\n",
    "    model_kwargs={'device': 'cuda'},  # GPU 사용\n",
    "    encode_kwargs={'normalize_embeddings': True}  # 임베딩 정규화\n",
    ")\n",
    "\n",
    "vectorstore = Chroma.from_texts(to_vectorize, embeddings, metadatas=examples)\n",
    "\n",
    "# 예제 선택기 생성\n",
    "example_selector = SemanticSimilarityExampleSelector(\n",
    "    vectorstore=vectorstore,\n",
    "    k=2,\n",
    ")\n",
    "\n",
    "# Few-shot 프롬프트 템플릿 생성\n",
    "few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
    "    example_selector=example_selector,\n",
    "    example_prompt=ChatPromptTemplate.from_messages(\n",
    "        [(\"human\", \"{input}\"), (\"ai\", \"{output}\")]\n",
    "    ),\n",
    ")\n",
    "\n",
    "# 최종 프롬프트 템플릿 생성\n",
    "final_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"당신은 과학과 수학에 대해 잘 아는 교육자입니다.\"),\n",
    "        few_shot_prompt,\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# 모델과 체인 생성\n",
    "chain = final_prompt | ChatOllama(model=\"Llama-3-Open-Ko-8B:latest\")\n",
    "\n",
    "result = chain.invoke(\"태양계에서 가장 큰 행성은 무엇인가요?\")\n",
    "print(result.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'input'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 58\u001b[0m\n\u001b[0;32m     55\u001b[0m chain \u001b[38;5;241m=\u001b[39m final_prompt \u001b[38;5;241m|\u001b[39m ChatOllama(model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLlama-3-Open-Ko-8B:latest\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# 체인 실행\u001b[39;00m\n\u001b[1;32m---> 58\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mchain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m태양계에서 가장 큰 행성은 무엇인가요?\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28mprint\u001b[39m(result\u001b[38;5;241m.\u001b[39mcontent)\n",
      "File \u001b[1;32mc:\\langchain\\venv\\lib\\site-packages\\langchain_core\\runnables\\base.py:3020\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m   3018\u001b[0m context\u001b[38;5;241m.\u001b[39mrun(_set_config_context, config)\n\u001b[0;32m   3019\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m-> 3020\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mrun(step\u001b[38;5;241m.\u001b[39minvoke, \u001b[38;5;28minput\u001b[39m, config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   3021\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   3022\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mrun(step\u001b[38;5;241m.\u001b[39minvoke, \u001b[38;5;28minput\u001b[39m, config)\n",
      "File \u001b[1;32mc:\\langchain\\venv\\lib\\site-packages\\langchain_core\\prompts\\base.py:208\u001b[0m, in \u001b[0;36mBasePromptTemplate.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtags:\n\u001b[0;32m    207\u001b[0m     config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtags\n\u001b[1;32m--> 208\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_with_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    209\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_format_prompt_with_error_handling\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    210\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    211\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    212\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrun_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprompt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    213\u001b[0m \u001b[43m    \u001b[49m\u001b[43mserialized\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_serialized\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    214\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\langchain\\venv\\lib\\site-packages\\langchain_core\\runnables\\base.py:1925\u001b[0m, in \u001b[0;36mRunnable._call_with_config\u001b[1;34m(self, func, input, config, run_type, serialized, **kwargs)\u001b[0m\n\u001b[0;32m   1921\u001b[0m     context \u001b[38;5;241m=\u001b[39m copy_context()\n\u001b[0;32m   1922\u001b[0m     context\u001b[38;5;241m.\u001b[39mrun(_set_config_context, child_config)\n\u001b[0;32m   1923\u001b[0m     output \u001b[38;5;241m=\u001b[39m cast(\n\u001b[0;32m   1924\u001b[0m         Output,\n\u001b[1;32m-> 1925\u001b[0m         context\u001b[38;5;241m.\u001b[39mrun(\n\u001b[0;32m   1926\u001b[0m             call_func_with_variable_args,  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m   1927\u001b[0m             func,  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m   1928\u001b[0m             \u001b[38;5;28minput\u001b[39m,  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m   1929\u001b[0m             config,\n\u001b[0;32m   1930\u001b[0m             run_manager,\n\u001b[0;32m   1931\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   1932\u001b[0m         ),\n\u001b[0;32m   1933\u001b[0m     )\n\u001b[0;32m   1934\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1935\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[1;32mc:\\langchain\\venv\\lib\\site-packages\\langchain_core\\runnables\\config.py:396\u001b[0m, in \u001b[0;36mcall_func_with_variable_args\u001b[1;34m(func, input, config, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    394\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m accepts_run_manager(func):\n\u001b[0;32m    395\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m run_manager\n\u001b[1;32m--> 396\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\langchain\\venv\\lib\\site-packages\\langchain_core\\prompts\\base.py:183\u001b[0m, in \u001b[0;36mBasePromptTemplate._format_prompt_with_error_handling\u001b[1;34m(self, inner_input)\u001b[0m\n\u001b[0;32m    181\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_format_prompt_with_error_handling\u001b[39m(\u001b[38;5;28mself\u001b[39m, inner_input: \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m PromptValue:\n\u001b[0;32m    182\u001b[0m     _inner_input \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_input(inner_input)\n\u001b[1;32m--> 183\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformat_prompt(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m_inner_input)\n",
      "File \u001b[1;32mc:\\langchain\\venv\\lib\\site-packages\\langchain_core\\prompts\\chat.py:785\u001b[0m, in \u001b[0;36mBaseChatPromptTemplate.format_prompt\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    776\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mformat_prompt\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m PromptValue:\n\u001b[0;32m    777\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Format prompt. Should return a PromptValue.\u001b[39;00m\n\u001b[0;32m    778\u001b[0m \n\u001b[0;32m    779\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    783\u001b[0m \u001b[38;5;124;03m        PromptValue.\u001b[39;00m\n\u001b[0;32m    784\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 785\u001b[0m     messages \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformat_messages(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ChatPromptValue(messages\u001b[38;5;241m=\u001b[39mmessages)\n",
      "File \u001b[1;32mc:\\langchain\\venv\\lib\\site-packages\\langchain_core\\prompts\\chat.py:1228\u001b[0m, in \u001b[0;36mChatPromptTemplate.format_messages\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m   1224\u001b[0m     result\u001b[38;5;241m.\u001b[39mextend([message_template])\n\u001b[0;32m   1225\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[0;32m   1226\u001b[0m     message_template, (BaseMessagePromptTemplate, BaseChatPromptTemplate)\n\u001b[0;32m   1227\u001b[0m ):\n\u001b[1;32m-> 1228\u001b[0m     message \u001b[38;5;241m=\u001b[39m message_template\u001b[38;5;241m.\u001b[39mformat_messages(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1229\u001b[0m     result\u001b[38;5;241m.\u001b[39mextend(message)\n\u001b[0;32m   1230\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\langchain\\venv\\lib\\site-packages\\langchain_core\\prompts\\few_shot.py:391\u001b[0m, in \u001b[0;36mFewShotChatMessagePromptTemplate.format_messages\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    389\u001b[0m \u001b[38;5;66;03m# Get the examples to use.\u001b[39;00m\n\u001b[0;32m    390\u001b[0m examples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_examples(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m--> 391\u001b[0m examples \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    392\u001b[0m     {k: e[k] \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexample_prompt\u001b[38;5;241m.\u001b[39minput_variables} \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m examples\n\u001b[0;32m    393\u001b[0m ]\n\u001b[0;32m    394\u001b[0m \u001b[38;5;66;03m# Format the examples.\u001b[39;00m\n\u001b[0;32m    395\u001b[0m messages \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    396\u001b[0m     message\n\u001b[0;32m    397\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m example \u001b[38;5;129;01min\u001b[39;00m examples\n\u001b[0;32m    398\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m message \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexample_prompt\u001b[38;5;241m.\u001b[39mformat_messages(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mexample)\n\u001b[0;32m    399\u001b[0m ]\n",
      "File \u001b[1;32mc:\\langchain\\venv\\lib\\site-packages\\langchain_core\\prompts\\few_shot.py:392\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    389\u001b[0m \u001b[38;5;66;03m# Get the examples to use.\u001b[39;00m\n\u001b[0;32m    390\u001b[0m examples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_examples(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    391\u001b[0m examples \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m--> 392\u001b[0m     {k: e[k] \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexample_prompt\u001b[38;5;241m.\u001b[39minput_variables} \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m examples\n\u001b[0;32m    393\u001b[0m ]\n\u001b[0;32m    394\u001b[0m \u001b[38;5;66;03m# Format the examples.\u001b[39;00m\n\u001b[0;32m    395\u001b[0m messages \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    396\u001b[0m     message\n\u001b[0;32m    397\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m example \u001b[38;5;129;01min\u001b[39;00m examples\n\u001b[0;32m    398\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m message \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexample_prompt\u001b[38;5;241m.\u001b[39mformat_messages(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mexample)\n\u001b[0;32m    399\u001b[0m ]\n",
      "File \u001b[1;32mc:\\langchain\\venv\\lib\\site-packages\\langchain_core\\prompts\\few_shot.py:392\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    389\u001b[0m \u001b[38;5;66;03m# Get the examples to use.\u001b[39;00m\n\u001b[0;32m    390\u001b[0m examples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_examples(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    391\u001b[0m examples \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m--> 392\u001b[0m     {k: \u001b[43me\u001b[49m\u001b[43m[\u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexample_prompt\u001b[38;5;241m.\u001b[39minput_variables} \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m examples\n\u001b[0;32m    393\u001b[0m ]\n\u001b[0;32m    394\u001b[0m \u001b[38;5;66;03m# Format the examples.\u001b[39;00m\n\u001b[0;32m    395\u001b[0m messages \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    396\u001b[0m     message\n\u001b[0;32m    397\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m example \u001b[38;5;129;01min\u001b[39;00m examples\n\u001b[0;32m    398\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m message \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexample_prompt\u001b[38;5;241m.\u001b[39mformat_messages(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mexample)\n\u001b[0;32m    399\u001b[0m ]\n",
      "\u001b[1;31mKeyError\u001b[0m: 'input'"
     ]
    }
   ],
   "source": [
    "from langchain_chroma import Chroma\n",
    "from langchain_core.example_selectors import SemanticSimilarityExampleSelector\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.prompts import ChatPromptTemplate, FewShotChatMessagePromptTemplate\n",
    "from langchain.chat_models import ChatOllama\n",
    "import torch\n",
    "# 예제 데이터 추가\n",
    "examples = [\n",
    "    {\"input\": \"지구의 대기 중 가장 많은 비율을 차지하는 기체는 무엇인가요?\", \"output\": \"질소입니다.\"},\n",
    "    {\"input\": \"광합성에 필요한 주요 요소들은 무엇인가요?\", \"output\": \"빛, 이산화탄소, 물입니다.\"},\n",
    "    {\"input\": \"피타고라스 정리를 설명해주세요.\", \"output\": \"직각삼각형에서 빗변의 제곱은 다른 두 변의 제곱의 합과 같습니다.\"},\n",
    "    {\"input\": \"DNA의 기본 구조를 간단히 설명해주세요.\", \"output\": \"DNA는 이중 나선 구조를 가진 핵산입니다.\"},\n",
    "    {\"input\": \"원주율(π)의 정의는 무엇인가요?\", \"output\": \"원의 둘레와 지름의 비율입니다.\"},\n",
    "]\n",
    "\n",
    "# 벡터화할 텍스트 생성\n",
    "to_vectorize = [\" \".join(example.values()) for example in examples]\n",
    "\n",
    "# Hugging Face 임베딩 모델 초기화\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"BAAI/bge-m3\",  # 모델 이름\n",
    "    model_kwargs={\"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\"},  # GPU/CPU 자동 선택\n",
    "    encode_kwargs={\"normalize_embeddings\": True}  # 정규화 활성화\n",
    ")\n",
    "\n",
    "# Chroma 벡터스토어 생성\n",
    "vectorstore = Chroma.from_texts(to_vectorize, embeddings, metadatas=examples)\n",
    "\n",
    "# 예제 선택기 생성\n",
    "example_selector = SemanticSimilarityExampleSelector(\n",
    "    vectorstore=vectorstore,\n",
    "    k=2  # 가장 유사한 2개의 예제 선택\n",
    ")\n",
    "\n",
    "# Few-shot 프롬프트 템플릿 생성\n",
    "example_prompt = ChatPromptTemplate.from_messages(\n",
    "    [(\"human\", \"{input}\"), (\"ai\", \"{output}\")]\n",
    ")\n",
    "\n",
    "few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
    "    example_selector=example_selector,\n",
    "    example_prompt=example_prompt\n",
    ")\n",
    "\n",
    "# 최종 프롬프트 템플릿 생성\n",
    "final_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"당신은 과학과 수학에 대해 잘 아는 교육자입니다.\"),\n",
    "        few_shot_prompt,\n",
    "        (\"human\", \"{input}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "# 모델과 체인 생성\n",
    "chain = final_prompt | ChatOllama(model=\"Llama-3-Open-Ko-8B:latest\")\n",
    "\n",
    "# 체인 실행\n",
    "result = chain.invoke(\"태양계에서 가장 큰 행성은 무엇인가요?\")\n",
    "print(result.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "지구의 지각에서 가장 흔한 원소는 산소입니다.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "# 기본 프롬프트 템플릿 정의\n",
    "prompt = PromptTemplate.from_template(\"지구의 {layer}에서 가장 흔한 원소는 {element}입니다.\")\n",
    "\n",
    "# 'layer' 변수에 '지각' 값을 미리 지정하여 부분 포맷팅\n",
    "partial_prompt = prompt.partial(layer=\"지각\")\n",
    "\n",
    "# 나머지 'element' 변수만 입력하여 완전한 문장 생성\n",
    "print(partial_prompt.format(element=\"산소\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "지구의 맨틀에서 가장 흔한 원소는 규소입니다.\n"
     ]
    }
   ],
   "source": [
    "# 프롬프트 초기화 시 부분 변수 지정\n",
    "prompt = PromptTemplate(\n",
    "    template=\"지구의 {layer}에서 가장 흔한 원소는 {element}입니다.\",\n",
    "    input_variables=[\"element\"],  # 사용자 입력이 필요한 변수\n",
    "    partial_variables={\"layer\": \"맨틀\"}  # 미리 지정된 부분 변수\n",
    ")\n",
    "\n",
    "# 남은 'element' 변수만 입력하여 문장 생성\n",
    "print(prompt.format(element=\"규소\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "겨울에 일어나는 대표적인 지구과학 현상은 꽃가루 증가입니다.\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# 현재 계절을 반환하는 함수 정의\n",
    "def get_current_season():\n",
    "    month = datetime.now().month\n",
    "    if 3 <= month <= 5:\n",
    "        return \"봄\"\n",
    "    elif 6 <= month <= 8:\n",
    "        return \"여름\"\n",
    "    elif 9 <= month <= 11:\n",
    "        return \"가을\"\n",
    "    else:\n",
    "        return \"겨울\"\n",
    "\n",
    "# 함수를 사용한 부분 변수가 있는 프롬프트 템플릿 정의\n",
    "prompt = PromptTemplate(\n",
    "    template=\"{season}에 일어나는 대표적인 지구과학 현상은 {phenomenon}입니다.\",\n",
    "    input_variables=[\"phenomenon\"],  # 사용자 입력이 필요한 변수\n",
    "    partial_variables={\"season\": get_current_season}  # 함수를 통해 동적으로 값을 생성하는 부분 변수\n",
    ")\n",
    "\n",
    "# 'phenomenon' 변수만 입력하여 현재 계절에 맞는 문장 생성\n",
    "print(prompt.format(phenomenon=\"꽃가루 증가\"))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
