{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1-4. LangChain의 언어 모델(Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Public\\Documents\\ESTsoft\\CreatorTemp\\ipykernel_16452\\138084570.py:2: LangChainDeprecationWarning: The class `ChatOllama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import ChatOllama``.\n",
      "  llm = ChatOllama(model=\"Llama-3-Open-Ko-8B:latest\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='서울의 명동, 부산의 해운대, 제주의 성산일출봉이 있습니다.\\r\\n명동은 한국의 대표적인 쇼핑거리로 유명합니다.\\r\\n해운대는 아름다운 바다와 함께 다양한 레스토랑과 카페가 즐비한 곳입니다.\\r\\n성산일출봉은 일출을 보기 위해 많은 관광객들이 찾는 곳입니다.', additional_kwargs={}, response_metadata={'model': 'Llama-3-Open-Ko-8B:latest', 'created_at': '2025-01-07T06:44:26.808355Z', 'message': {'role': 'assistant', 'content': ''}, 'done_reason': 'stop', 'done': True, 'total_duration': 30767000600, 'load_duration': 18903378900, 'prompt_eval_count': 68, 'prompt_eval_duration': 983000000, 'eval_count': 90, 'eval_duration': 10459000000}, id='run-b9dc2096-0eec-4e9c-8cf4-ac234ee842c7-0')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.chat_models import ChatOllama\n",
    "llm = ChatOllama(model=\"Llama-3-Open-Ko-8B:latest\")\n",
    "\n",
    "llm.invoke(\"한국의 대표적인 관광지 3군데를 추천해주세요.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1-1-4-2. Chat Model\n",
    "- ChatPromptTemplate를 사용하여 대화형 프롬프트를 생성\n",
    "\n",
    "Chat Model 인터페이스의 특징\n",
    "- 대화형 입력과 출력\n",
    "- 다양한 모델 제공 업체와의 통합\n",
    "- 다양한 작동 모드 지원\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='한국에는 많은 관광지가 있지만, 가장 인기 있는 곳을 알려드리겠습니다. 먼저, 경복궁은 조선시대 왕이 살던 궁궐로, 우리나라의 전통 건축양식을 볼 수 있습니다. 다음으로, 남산은 서울 도심 한가운데에 위치한 산으로, 다양한 문화공간과 전망대가 있어 많은 관광객들이 찾고 있습니다. 마지막으로, 북촌 한옥마을은 조선시대의 가옥이 잘 보존되어 있는 곳으로, 한국의 전통문화를 체험할 수 있습니다.', additional_kwargs={}, response_metadata={'model': 'Llama-3-Open-Ko-8B:latest', 'created_at': '2025-01-07T06:47:57.9322584Z', 'message': {'role': 'assistant', 'content': ''}, 'done_reason': 'stop', 'done': True, 'total_duration': 14783490700, 'load_duration': 17125200, 'prompt_eval_count': 50, 'prompt_eval_duration': 516000000, 'eval_count': 125, 'eval_duration': 14248000000}, id='run-4124d450-d330-4a69-98ae-d40eae454884-0')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "chat = ChatOllama(model=\"Llama-3-Open-Ko-8B:latest\")\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\",\"이 시스템은 여행 전문가입니다.\"),\n",
    "    (\"user\",\"{user_input}\"),\n",
    "])\n",
    "\n",
    "chain = chat_prompt | chat\n",
    "chain.invoke({\"user_input\": \"안녕하세요? 한국의 대표적인 관광지 3군데를 추천해주세요.\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-4-2. LangChain의 LLM 모델 파라미터 설정\n",
    "\n",
    "- Temperature: 생성된 텍스트의 다양성을 조정. 값이 작으면 예측 가능하고 일관된 출력을 생성하는 반면, 값이 크면 다양하고 예측하기 어려운 출력을 생성\n",
    "- Max Tokens (최대 토큰 수): 생성할 최대 토큰 수를 지정. 생성할 텍스트의 길이를 제한.\n",
    "- Top P (Top Probability): 생성 과정에서 특정 확률 분포 내에서 상위 P% 토큰만을 고려하는 방식. 이는 출력의 다양성을 조정하는 데 도움이 됨.\n",
    "- Frequency Penalty (빈도 패널티): 값이 클수록 이미 등장한 단어나 구절이 다시 등장할 확률을 감소. 이를 통해 반복을 줄이고 텍스트의 다양성을 증가시킬 수 있음. (0~1)\n",
    "- Presence Penalty (존재 패널티): 텍스트 내에서 단어의 존재 유무에 따라 그 단어의 선택 확률을 조정. 값이 클수록 아직 텍스트에 등장하지 않은 새로운 단어의 사용이 장려. (0~1)\n",
    "- Stop Sequences (정지 시퀀스): 특정 단어나 구절이 등장할 경우 생성을 멈추도록 설정. 이는 출력을 특정 포인트에서 종료하고자 할 때 사용."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='태양계의 행성 중 가장 크고 무거운 것은 목성이에요. 지구보다 10배나 더 큽니다. 목성은 태양계 전체 질량의 약 1/3을 차지하고 있어요.' additional_kwargs={} response_metadata={'model': 'Llama-3-Open-Ko-8B:latest', 'created_at': '2025-01-07T06:52:29.4184381Z', 'message': {'role': 'assistant', 'content': ''}, 'done_reason': 'stop', 'done': True, 'total_duration': 6079269200, 'load_duration': 17476000, 'prompt_eval_count': 65, 'prompt_eval_duration': 411000000, 'eval_count': 55, 'eval_duration': 5648000000} id='run-17c5d6c1-a998-4dbb-9198-7b1bc5504cd0-0'\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.chat_models import ChatOllama\n",
    "\n",
    "params = {\n",
    "    \"temperature\": 0.7,         # 생성된 텍스트의 다양성 조정\n",
    "    \"max_tokens\": 100,          # 생성할 최대 토큰 수    \n",
    "}\n",
    "\n",
    "kwargs = {\n",
    "    \"frequency_penalty\": 0.5,   # 이미 등장한 단어의 재등장 확률\n",
    "    \"presence_penalty\": 0.5,    # 새로운 단어의 도입을 장려\n",
    "    \"stop\": [\"\\n\"]              # 정지 시퀀스 설정\n",
    "\n",
    "}\n",
    "\n",
    "# 모델 인스턴스를 생성할 때 설정\n",
    "model =  ChatOllama(model=\"Llama-3-Open-Ko-8B:latest\")\n",
    "\n",
    "\n",
    "# 모델 호출\n",
    "question = \"태양계에서 가장 큰 행성은 무엇인가요?\"\n",
    "response = model.invoke(input=question)\n",
    "\n",
    "# 전체 응답 출력\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "태양계의 행성을 크기 순으로 나열하면 다음과 같습니다. 지구, 화성, 목성, 토성, 천왕성, 해왕성이 있습니다.\n"
     ]
    }
   ],
   "source": [
    "# 모델 파라미터 설정\n",
    "params = {\n",
    "    \"temperature\": 0.7,         # 생성된 텍스트의 다양성 조정\n",
    "    \"max_tokens\": 10,          # 생성할 최대 토큰 수    \n",
    "}\n",
    "\n",
    "# 모델 인스턴스를 호출할 때 전달\n",
    "response = model.invoke(input=question, **params)\n",
    "\n",
    "# 문자열 출력\n",
    "print(response.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-4-2-2. LLM 모델 파라미터를 추가로 바인딩 (bind 메소드)\n",
    "\n",
    "bind 메소드를 사용하여 모델 인스턴스에 파라미터를 추가로 제공   \n",
    "장점은 특정 모델 설정을 기본값으로 사용하고자 할 때 유용하며, 특수한 상황에서 일부 파라미터를 다르게 적용하고 싶을 때 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='지구입니다. 지구는 태양계에서 가장 질량이 크고 밀도가 높은 행성이죠.' additional_kwargs={} response_metadata={'model': 'Llama-3-Open-Ko-8B:latest', 'created_at': '2025-01-07T06:56:52.5857204Z', 'message': {'role': 'assistant', 'content': ''}, 'done_reason': 'stop', 'done': True, 'total_duration': 2859352900, 'load_duration': 16853100, 'prompt_eval_count': 49, 'prompt_eval_duration': 366000000, 'eval_count': 26, 'eval_duration': 2475000000} id='run-13108894-3fec-40df-80fc-c4166ba7a1b0-0'\n",
      "content='지구입니다. 지구는 태양계에서 네 번째로 크고, 목성보다 약 10배 더 무겁습니다.' additional_kwargs={} response_metadata={'model': 'Llama-3-Open-Ko-8B:latest', 'created_at': '2025-01-07T06:56:58.6331641Z', 'message': {'role': 'assistant', 'content': ''}, 'done_reason': 'stop', 'done': True, 'total_duration': 4016787400, 'load_duration': 16881300, 'prompt_eval_count': 49, 'prompt_eval_duration': 140000000, 'eval_count': 32, 'eval_duration': 3858000000} id='run-86ba4e80-0806-46d6-b5bb-c1bb5dc6d6c4-0'\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"이 시스템은 천문학 질문에 답변할 수 있습니다.\"),\n",
    "    (\"user\", \"{user_input}\"),\n",
    "])\n",
    "\n",
    "model =  ChatOllama(model=\"Llama-3-Open-Ko-8B:latest\",max_tokens=100)\n",
    "\n",
    "messages = prompt.format_messages(user_input=\"태양계에서 가장 큰 행성은 무엇인가요?\")\n",
    "\n",
    "before_answer = model.invoke(messages)\n",
    "\n",
    "print(before_answer)\n",
    "\n",
    "chain = prompt | model.bind(max_tokens=10)\n",
    "\n",
    "after_answer = chain.invoke({\"user_input\": \"태양계에서 가장 큰 행성은 무엇인가요?\"})\n",
    "\n",
    "print(after_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1-5. 출력 파서(Output Parser)\n",
    "- 출력 포맷 변경\n",
    "- 정보 추출\n",
    "- 결과 정제\n",
    "- 조건부 로직 적용\n",
    "\n",
    "\n",
    "출력 파서 (Output Parser)의 사용 사례\n",
    "자연어 처리(NLP) 애플리케이션   \n",
    "데이터 분석   \n",
    "챗봇 개발   \n",
    "콘텐츠 생성   \n",
    "\n",
    "### 1-5-1. CSV Parser\n",
    "- 랭체인의 CommaSeparatedListOutputParser를 사용하여, 모델이 생성한 텍스트에서 쉼표(,)로 구분된 항목을 추출하여 리스트 형태로 정리하여 파싱하는 과정\n",
    "- 생성 모델의 출력을 구조화된 데이터 형태로 변환할 수 있으며, 이후 처리 과정에서 데이터를 더 편리하게 활용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your response should be a list of comma separated values, eg: `foo, bar, baz` or `foo,bar,baz`\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import CommaSeparatedListOutputParser\n",
    "\n",
    "output_parser = CommaSeparatedListOutputParser()\n",
    "format_instructions = output_parser.get_format_instructions()\n",
    "\n",
    "print(format_instructions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PromptTemplate을 사용하여 사용자 입력(subject)에 기반한 프롬프트를 생성   \n",
    "프롬프트에는 사용자가 지정한 주제(subject)와 모델에 전달할 포맷 지시사항(format_instructions)이 포함   \n",
    "그리고 prompt, model, output_parser를 파이프(|) 연산자를 사용하여 연결하여 체인을 구성 -> 이 체인은 사용자의 입력을 받아 프롬프트를 생성하고, 생성된 프롬프트를 모델에 전달한 후, 모델의 출력을 파싱하는 과정을 순차적으로 수행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Here are 5 popular Korean cuisines:',\n",
       " 'Bibimbap',\n",
       " 'Japchae',\n",
       " 'Bulgogi',\n",
       " 'Kimchi Stew',\n",
       " 'Naengmyeon']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"List five {subject}.\\n{format_instructions}\",\n",
    "    input_variables=[\"subject\"],\n",
    "    partial_variables={\"format_instructions\": format_instructions},\n",
    ")\n",
    "\n",
    "llm =ChatOllama(model=\"llama3:latest\", temperature=0)\n",
    "\n",
    "chain = prompt | llm | output_parser\n",
    "\n",
    "chain.invoke({\"subject\": \"popular Korean cusine\"})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-5-2.JSON Parser\n",
    "\n",
    "- JsonOutputParser와 Pydantic을 사용하여, 모델 출력을 JSON 형식으로 파싱하고 Pydantic 모델로 구조화하는 과정을 설명\n",
    "- JsonOutputParser는 모델의 출력을 JSON으로 해석하고, 지정된 Pydantic 모델(CusineRecipe 클래스)에 맞게 데이터를 구조화하여 제공\n",
    "\n",
    "CusineRecipe 클래스를 Pydantic BaseModel을 사용하여 정의   \n",
    "출력 파서로 JsonOutputParser 인스턴스를 생성하고, pydantic_object 매개변수로 CusineRecipe 클래스를 전달   \n",
    "모델 출력을 해당 Pydantic 모델로 파싱하도록 설정   \n",
    "output_parser.get_format_instructions() 메소드를 호출하여 모델에 전달할 포맷 지시사항을 얻음   \n",
    "이 지시사항은 모델이 출력을 생성할 때 JSON 형식을 따르도록 안내하는 역할"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output should be formatted as a JSON instance that conforms to the JSON schema below.\n",
      "\n",
      "As an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\n",
      "the object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\n",
      "\n",
      "Here is the output schema:\n",
      "```\n",
      "{\"properties\": {\"name\": {\"title\": \"Name\", \"description\": \"name of a cusine\", \"type\": \"string\"}, \"recipe\": {\"title\": \"Recipe\", \"description\": \"recipe to cook the cusine\", \"type\": \"string\"}}, \"required\": [\"name\", \"recipe\"]}\n",
      "```\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\langchain\\venv\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3577: LangChainDeprecationWarning: As of langchain-core 0.3.0, LangChain uses pydantic v2 internally. The langchain_core.pydantic_v1 module was a compatibility shim for pydantic v1, and should no longer be used. Please update the code to import from Pydantic directly.\n",
      "\n",
      "For example, replace imports like: `from langchain_core.pydantic_v1 import BaseModel`\n",
      "with: `from pydantic import BaseModel`\n",
      "or the v1 compatibility namespace if you are working in a code base that has not been fully upgraded to pydantic 2 yet. \tfrom pydantic.v1 import BaseModel\n",
      "\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "# 자료구조 정의 (pydantic)\n",
    "class CusineRecipe(BaseModel):\n",
    "    name: str = Field(description=\"name of a cusine\")\n",
    "    recipe: str = Field(description=\"recipe to cook the cusine\")\n",
    "\n",
    "# 출력 파서 정의\n",
    "output_parser = JsonOutputParser(pydantic_object=CusineRecipe)\n",
    "\n",
    "format_instructions = output_parser.get_format_instructions()\n",
    "\n",
    "print(format_instructions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다음 단계는 모델에 입력으로 전달할 프롬프트를 구성하는 것   \n",
    "PromptTemplate을 사용하여 사용자 질문(query)을 기반으로 한 프롬프트를 생성  \n",
    "프롬프트에는 사용자의 질문과 모델에 전달할 포맷 지시사항이 포함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['query'] input_types={} partial_variables={'format_instructions': 'The output should be formatted as a JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output schema:\\n```\\n{\"properties\": {\"name\": {\"title\": \"Name\", \"description\": \"name of a cusine\", \"type\": \"string\"}, \"recipe\": {\"title\": \"Recipe\", \"description\": \"recipe to cook the cusine\", \"type\": \"string\"}}, \"required\": [\"name\", \"recipe\"]}\\n```'} template='Answer the user query.\\n{format_instructions}\\n{query}\\n'\n"
     ]
    }
   ],
   "source": [
    "# prompt 구성\n",
    "prompt = PromptTemplate(\n",
    "    template=\"Answer the user query.\\n{format_instructions}\\n{query}\\n\",\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\"format_instructions\": format_instructions},\n",
    ")\n",
    "\n",
    "print(prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'Bibimbap',\n",
       " 'recipe': 'To cook Bibimbap, start by cooking white rice. Then, heat some oil in a pan and add your choice of vegetables (such as zucchini, carrots, mushrooms, and bean sprouts). Add cooked beef or tofu for protein. In a separate bowl, whisk together two eggs and season with salt and pepper. Pour the egg mixture over the vegetables and cook until the eggs are set. Finally, place a scoop of rice in a bowl, add the vegetable and egg mixture on top, and serve with your favorite dipping sauce.'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model =ChatOllama(model=\"llama3:latest\", temperature=0)\n",
    "chain = prompt | model | output_parser\n",
    "\n",
    "chain.invoke({\"query\": \"Let me know how to cook Bibimbap\"})\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
